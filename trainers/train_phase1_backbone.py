# PNG ì½ì–´ì˜¤ê¸°
# G:/VETNet_pilot/trainers/train_phase1_backbone.py
import os, sys, time, numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from tqdm import tqdm
from PIL import Image

ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)

print("[DEBUG] Using ROOT:", ROOT)

from datasets.multitask_dataset_cache import MultiTaskDatasetCache
from models.backbone.vetnet_backbone import VETNetBackbone
from torch.amp import autocast, GradScaler

try:
    from skimage.metrics import peak_signal_noise_ratio, structural_similarity
    USE_SKIMAGE = True
except:
    USE_SKIMAGE = False


# ============================================================
class Config:
    cache_root = "E:/VETNet_Pilot/preload_cache"

    save_root = "E:/VETNet_pilot/checkpoints/phase1_backbone"
    results_root = "E:/VETNet_pilot/results/phase1_backbone"

    epochs = 100
    batch_size = 2
    num_workers = 0
    lr = 3e-4

    in_channels = 3
    out_channels = 3
    dim = 64
    num_blocks = (4, 6, 6, 8)
    heads = (1, 2, 4, 8)
    volterra_rank = 4
    ffn_expansion_factor = 2.66
    bias = False

    metric_images_per_batch = 2
    use_amp = True

    preview_count = 3
    iter_save_interval = 150   # ğŸ”µ ì¶”ê°€: iteration ì €ì¥ ì£¼ê¸°


cfg = Config()


# ============================================================
def tensor_to_img(t):
    t = t.detach().cpu().clamp(0, 1).permute(1, 2, 0).numpy()
    return (t * 255).astype("uint8")


def save_triplet(input, pred, gt, path):
    inp = tensor_to_img(input)
    pr = tensor_to_img(pred)
    gt_img = tensor_to_img(gt)

    H, W, _ = inp.shape
    canvas = np.zeros((H, W * 3, 3), dtype=np.uint8)
    canvas[:, 0:W] = inp
    canvas[:, W:2*W] = pr
    canvas[:, 2*W:3*W] = gt_img

    os.makedirs(os.path.dirname(path), exist_ok=True)
    Image.fromarray(canvas).save(path)


def save_preview_images(inputs, preds, gts, epoch, save_dir, count=3):
    os.makedirs(save_dir, exist_ok=True)

    total = inputs.size(0)
    count = min(count, total)
    idxs = np.random.choice(total, count, replace=False)

    for i, idx in enumerate(idxs):
        path = os.path.join(save_dir, f"epoch_{epoch:03d}_preview_{i:02d}.png")
        save_triplet(inputs[idx], preds[idx], gts[idx], path)


def compute_psnr_ssim(pred, gt):
    if not USE_SKIMAGE:
        return 0, 0
    p = tensor_to_img(pred[0])
    g = tensor_to_img(gt[0])
    psnr = peak_signal_noise_ratio(g, p, data_range=255)
    ssim = structural_similarity(g, p, channel_axis=2)
    return psnr, ssim


# ============================================================
def train_phase1():

    os.makedirs(cfg.save_root, exist_ok=True)
    os.makedirs(cfg.results_root, exist_ok=True)
    os.makedirs(os.path.join(cfg.results_root, "iter"), exist_ok=True)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print("[Device]", device)

    dataset = MultiTaskDatasetCache(cfg.cache_root, size=256)
    loader = DataLoader(
        dataset,
        batch_size=cfg.batch_size,
        shuffle=True,
        num_workers=0,
        pin_memory=True,
        drop_last=True,
    )

    print("[Data] Total cached samples =", len(dataset))

    model = VETNetBackbone(
        in_channels=cfg.in_channels,
        out_channels=cfg.out_channels,
        dim=cfg.dim,
        num_blocks=cfg.num_blocks,
        heads=cfg.heads,
        volterra_rank=cfg.volterra_rank,
        ffn_expansion_factor=cfg.ffn_expansion_factor,
        bias=cfg.bias,
    ).to(device)

    print("[Model Params]", sum(p.numel() for p in model.parameters()) / 1e6, "M")

    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.epochs)
    scaler = GradScaler()

    best_ssim = -1

    # ============================================================
    for epoch in range(1, cfg.epochs + 1):

        model.train()
        loss_sum = 0
        psnr_sum = 0
        ssim_sum = 0
        cnt = 0

        pbar = tqdm(loader, ncols=120, desc=f"Epoch {epoch}")

        preview_inp = None
        preview_gt = None
        preview_pred = None

        for it, batch in enumerate(pbar, start=1):

            inp = batch["input"].to(device)
            gt = batch["gt"].to(device)

            optimizer.zero_grad(set_to_none=True)

            with autocast(device_type="cuda", dtype=torch.float16, enabled=cfg.use_amp):
                pred = model(inp)
                loss = F.l1_loss(pred, gt)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            pred_c = pred.clamp(0, 1)

            if preview_inp is None:
                preview_inp = inp.detach().cpu()
                preview_gt = gt.detach().cpu()
                preview_pred = pred_c.detach().cpu()

            # ğŸ”µ 150 iterationë§ˆë‹¤ ì €ì¥
            if it % cfg.iter_save_interval == 0:
                iter_path = os.path.join(
                    cfg.results_root,
                    "iter",
                    f"epoch_{epoch:03d}_iter_{it:05d}.png"
                )
                save_triplet(inp[0], pred_c[0], gt[0], iter_path)

            ps, ss = compute_psnr_ssim(pred_c, gt)

            loss_sum += loss.item()
            psnr_sum += ps
            ssim_sum += ss
            cnt += 1

            pbar.set_postfix({
                "L": f"{loss_sum/cnt:.4f}",
                "P": f"{psnr_sum/cnt:.2f}",
                "S": f"{ssim_sum/cnt:.3f}",
            })

        epoch_loss = loss_sum / cnt
        epoch_psnr = psnr_sum / cnt
        epoch_ssim = ssim_sum / cnt

        scheduler.step()

        print(f"\n[Epoch {epoch}] Loss={epoch_loss:.4f}  PSNR={epoch_psnr:.2f}  SSIM={epoch_ssim:.4f}")

        save_preview_images(
            preview_inp, preview_pred, preview_gt,
            epoch, cfg.results_root, count=cfg.preview_count
        )

        img_path = os.path.join(
            cfg.results_root,
            f"epoch_{epoch:03d}_L{epoch_loss:.4f}_P{epoch_psnr:.2f}_S{epoch_ssim:.4f}.png",
        )
        save_triplet(preview_inp[0], preview_pred[0], preview_gt[0], img_path)

        ckpt_path = os.path.join(
            cfg.save_root,
            f"epoch_{epoch:03d}_L{epoch_loss:.4f}_P{epoch_psnr:.2f}_S{epoch_ssim:.4f}.pth",
        )
        torch.save(
            {
                "epoch": epoch,
                "state_dict": model.state_dict(),
                "optimizer": optimizer.state_dict(),
                "scheduler": scheduler.state_dict(),
            },
            ckpt_path,
        )

        if epoch_ssim > best_ssim:
            best_ssim = epoch_ssim
            torch.save(model.state_dict(), os.path.join(cfg.save_root, "best_phase1_backbone.pth"))
            print("[BEST] Updated best SSIM model")

    print("\nTraining Finished.")
    print("Best SSIM:", best_ssim)


if __name__ == "__main__":
    train_phase1()


# ì´ì–´ì„œ í•™ìŠµ
# G:/VETNet_pilot/trainers/train_phase1_backbone.py
""" import os, sys, time, numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from tqdm import tqdm
from PIL import Image
import re # ì •ê·œ í‘œí˜„ì‹ ëª¨ë“ˆ ì¶”ê°€

ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)

print("[DEBUG] Using ROOT:", ROOT)

from datasets.multitask_dataset_cache import MultiTaskDatasetCache
from models.backbone.vetnet_backbone import VETNetBackbone
from torch.cuda.amp import autocast, GradScaler # torch.amp ëŒ€ì‹  torch.cuda.amp ì‚¬ìš© ê¶Œì¥

try:
    from skimage.metrics import peak_signal_noise_ratio, structural_similarity
    USE_SKIMAGE = True
except:
    USE_SKIMAGE = False


# ============================================================
class Config:
    cache_root = "G:/VETNet_pilot/preload_cache"

    save_root = "G:/VETNet_pilot/checkpoints/phase1_backbone"
    results_root = "G:/VETNet_pilot/results/phase1_backbone"

    epochs = 100 # ì „ì²´ ëª©í‘œ ì—í¬í¬ (ì´ì–´ì„œ í›ˆë ¨ ì‹œ ì‹œì‘ ì—í¬í¬ì— ë”°ë¼ ì‹¤ì œ í›ˆë ¨ íšŸìˆ˜ ê²°ì •)
    batch_size = 2
    num_workers = 0 
    lr = 3e-4

    in_channels = 3
    out_channels = 3
    dim = 64
    num_blocks = (4, 6, 6, 8)
    heads = (1, 2, 4, 8)
    volterra_rank = 4
    ffn_expansion_factor = 2.66
    bias = False

    metric_images_per_batch = 2
    use_amp = True

    # ğŸ”µ ìƒˆë¡œ ì¶”ê°€: ë¯¸ë¦¬ë³´ê¸°ë¡œ ì €ì¥í•  ì´ë¯¸ì§€ ìˆ˜
    preview_count = 3


cfg = Config()


# ============================================================
def tensor_to_img(t):
    # detach(), cpu(), clamp(0, 1), permute(1, 2, 0) (C, H, W -> H, W, C), numpy()
    t = t.detach().cpu().clamp(0, 1).permute(1, 2, 0).numpy()
    return (t * 255).astype("uint8")


def save_triplet(input, pred, gt, path):
    inp = tensor_to_img(input)
    pr = tensor_to_img(pred)
    gt_img = tensor_to_img(gt)

    H, W, _ = inp.shape
    # Input | Prediction | Ground Truth ìˆœì„œë¡œ ì´ë¯¸ì§€ë¥¼ ë‚˜ì—´
    canvas = np.zeros((H, W * 3, 3), dtype=np.uint8)
    canvas[:, 0:W] = inp
    canvas[:, W:2*W] = pr
    canvas[:, 2*W:3*W] = gt_img

    os.makedirs(os.path.dirname(path), exist_ok=True)
    Image.fromarray(canvas).save(path)


# ğŸ”µ ëœë¤ ë¯¸ë¦¬ë³´ê¸° ì €ì¥ ê¸°ëŠ¥
def save_preview_images(inputs, preds, gts, epoch, save_dir, count=3):
    os.makedirs(save_dir, exist_ok=True)

    total = inputs.size(0)
    count = min(count, total)

    # ëœë¤ ì„ íƒ
    idxs = np.random.choice(total, count, replace=False)

    for i, idx in enumerate(idxs):
        path = os.path.join(save_dir, f"epoch_{epoch:03d}_preview_{i:02d}.png")
        # idxëŠ” ëœë¤ìœ¼ë¡œ ì„ íƒëœ ë°°ì¹˜ ì¸ë±ìŠ¤
        save_triplet(inputs[idx], preds[idx], gts[idx], path)


def compute_psnr_ssim(pred, gt):
    if not USE_SKIMAGE:
        return 0, 0
    # ë°°ì¹˜ì—ì„œ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ì‚¬ìš©
    p = tensor_to_img(pred[0])
    g = tensor_to_img(gt[0])
    psnr = peak_signal_noise_ratio(g, p, data_range=255)
    # channel_axis=2ëŠ” (H, W, C) í˜•ì‹ì„ì„ ì§€ì •
    ssim = structural_similarity(g, p, channel_axis=2)
    return psnr, ssim


# ğŸ”µ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ í•¨ìˆ˜ (ì¶”ê°€)
def load_checkpoint(save_root, model, optimizer, scheduler):
    start_epoch = 1
    best_ssim = -1.0
    latest_ckpt_path = None
    latest_epoch = 0

    # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ëª©ë¡ ê²€ìƒ‰
    if os.path.exists(save_root):
        files = os.listdir(save_root)
        
        # 'epoch_XXX...' í˜•ì‹ì˜ íŒŒì¼ ì¤‘ ê°€ì¥ í° ì—í¬í¬ ë²ˆí˜¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        pattern = re.compile(r"epoch_(\d{3})_L.*\.pth")
        
        for file in files:
            match = pattern.match(file)
            if match:
                epoch = int(match.group(1))
                if epoch > latest_epoch:
                    latest_epoch = epoch
                    latest_ckpt_path = os.path.join(save_root, file)

    if latest_ckpt_path:
        print(f"\n[INFO] Latest checkpoint found: {latest_ckpt_path}")
        try:
            checkpoint = torch.load(latest_ckpt_path)
            
            # ëª¨ë¸ ìƒíƒœ ë¡œë“œ
            model.load_state_dict(checkpoint["state_dict"])
            
            # ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ë¡œë“œ
            if "optimizer" in checkpoint:
                optimizer.load_state_dict(checkpoint["optimizer"])
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ ë¡œë“œ
            if "scheduler" in checkpoint:
                scheduler.load_state_dict(checkpoint["scheduler"])
                
            # ì‹œì‘ ì—í¬í¬ ë° ìµœê³  SSIM ì—…ë°ì´íŠ¸
            if "epoch" in checkpoint:
                start_epoch = checkpoint["epoch"] + 1 # ë‹¤ìŒ ì—í¬í¬ë¶€í„° ì‹œì‘
            
            # íŒŒì¼ ì´ë¦„ì—ì„œ SSIM ê°’ì„ ì¶”ì¶œí•˜ì—¬ best_ssim ì—…ë°ì´íŠ¸ ì‹œë„
            ssim_match = re.search(r"S([\d\.]+)\.pth$", latest_ckpt_path)
            if ssim_match:
                best_ssim = float(ssim_match.group(1))
            
            print(f"[INFO] Resuming training from Epoch {start_epoch}, Current Best SSIM: {best_ssim:.4f}")
            
        except Exception as e:
            print(f"[ERROR] Failed to load checkpoint {latest_ckpt_path}: {e}")
            start_epoch = 1 # ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì—í¬í¬ 1ë¶€í„° ë‹¤ì‹œ ì‹œì‘
            best_ssim = -1.0
    else:
        print("[INFO] No previous checkpoint found. Starting training from Epoch 1.")

    return start_epoch, best_ssim


# ============================================================
def train_phase1():

    os.makedirs(cfg.save_root, exist_ok=True)
    os.makedirs(cfg.results_root, exist_ok=True)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print("[Device]", device)

    # ============================================================
    # 1. ë°ì´í„° ë¡œë” ì„¤ì •
    # ============================================================
    dataset = MultiTaskDatasetCache(cfg.cache_root, size=256)
    loader = DataLoader(
        dataset,
        batch_size=cfg.batch_size,
        shuffle=True,
        num_workers=cfg.num_workers, # num_workers=0 ì„¤ì • ì‚¬ìš©
        pin_memory=True,
        drop_last=True,
    )

    print("[Data] Total cached samples =", len(dataset))

    # ============================================================
    # 2. ëª¨ë¸, ì˜µí‹°ë§ˆì´ì €, ìŠ¤ì¼€ì¤„ëŸ¬, ìŠ¤ì¼€ì¼ëŸ¬ ì„¤ì •
    # ============================================================
    model = VETNetBackbone(
        in_channels=cfg.in_channels,
        out_channels=cfg.out_channels,
        dim=cfg.dim,
        num_blocks=cfg.num_blocks,
        heads=cfg.heads,
        volterra_rank=cfg.volterra_rank,
        ffn_expansion_factor=cfg.ffn_expansion_factor,
        bias=cfg.bias,
    ).to(device)

    print("[Model Params]", sum(p.numel() for p in model.parameters()) / 1e6, "M")

    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.epochs)
    scaler = GradScaler(enabled=cfg.use_amp) # GradScalerì— enabled ì¸ì ì¶”ê°€

    # ============================================================
    # 3. ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ì¶”ê°€ëœ ë¶€ë¶„)
    # ============================================================
    start_epoch, best_ssim = load_checkpoint(cfg.save_root, model, optimizer, scheduler)
    
    # í›ˆë ¨ ì‹œì‘ ì—í¬í¬ë¶€í„° ì „ì²´ ì—í¬í¬ê¹Œì§€ ë°˜ë³µ
    for epoch in range(start_epoch, cfg.epochs + 1):

        model.train()
        loss_sum = 0
        psnr_sum = 0
        ssim_sum = 0
        cnt = 0

        pbar = tqdm(loader, ncols=120, desc=f"Epoch {epoch}")

        # ğŸ”µ ë¯¸ë¦¬ë³´ê¸° ì €ì¥ìš© ì„ì‹œ ë²„í¼ (ìƒˆ ì—í¬í¬ë§ˆë‹¤ ì´ˆê¸°í™”)
        preview_inp = None
        preview_gt = None
        preview_pred = None

        for batch in pbar:
            inp = batch["input"].to(device) # ì…ë ¥ ì´ë¯¸ì§€ (ì €í™”ì§ˆ/ë…¸ì´ì¦ˆ ë“±)
            gt = batch["gt"].to(device)     # ì •ë‹µ ì´ë¯¸ì§€ (ê³ í™”ì§ˆ)

            optimizer.zero_grad(set_to_none=True)

            # AMP(ìë™ í˜¼í•© ì •ë°€ë„) ì‚¬ìš©
            with autocast(dtype=torch.float16, enabled=cfg.use_amp):
                pred = model(inp)
                loss = F.l1_loss(pred, gt)

            # ì—­ì „íŒŒ ë° ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ (GradScaler ì‚¬ìš©)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            pred_c = pred.clamp(0, 1) # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ 0~1 ì‚¬ì´ë¡œ í´ë¨í•‘

            # ğŸ”µ ë¯¸ë¦¬ë³´ê¸°ìš© ì²« ë°°ì¹˜ë¥¼ ì €ì¥
            if preview_inp is None:
                # í›ˆë ¨ ì¤‘ì´ì§€ë§Œ, detach/cpu í•˜ì—¬ ì´í›„ ì´ë¯¸ì§€ ì €ì¥ì— ì‚¬ìš©
                preview_inp = inp.detach().cpu()
                preview_gt = gt.detach().cpu()
                preview_pred = pred_c.detach().cpu()

            # í‰ê°€ ì§€í‘œ ê³„ì‚° (ì²« ë²ˆì§¸ ìƒ˜í”Œì— ëŒ€í•´ì„œë§Œ ê³„ì‚°)
            ps, ss = compute_psnr_ssim(pred_c, gt)

            loss_sum += loss.item()
            psnr_sum += ps
            ssim_sum += ss
            cnt += 1

            # tqdm ë§‰ëŒ€ì— í˜„ì¬ í‰ê·  ì§€í‘œ í‘œì‹œ
            pbar.set_postfix({
                "L": f"{loss_sum/cnt:.4f}", # L1 Loss í‰ê· 
                "P": f"{psnr_sum/cnt:.2f}", # PSNR í‰ê· 
                "S": f"{ssim_sum/cnt:.3f}", # SSIM í‰ê· 
            })

        epoch_loss = loss_sum / cnt
        epoch_psnr = psnr_sum / cnt
        epoch_ssim = ssim_sum / cnt

        scheduler.step() # ì—í¬í¬ ì¢…ë£Œ í›„ ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸

        print(f"\n[Epoch {epoch}] Loss={epoch_loss:.4f} Â PSNR={epoch_psnr:.2f} Â SSIM={epoch_ssim:.4f}")

        # ======================================================
        # ğŸ”µ ëœë¤ Preview ì´ë¯¸ì§€ ì €ì¥ (preview_count ìˆ˜ë§Œí¼)
        # ======================================================
        save_preview_images(preview_inp, preview_pred, preview_gt,
                             epoch, cfg.results_root, count=cfg.preview_count)

        # ======================================================
        # ëŒ€í‘œ ì´ë¯¸ì§€ ì €ì¥ (ì²« 1ì¥)
        # ======================================================
        img_path = os.path.join(
            cfg.results_root,
            f"epoch_{epoch:03d}_L{epoch_loss:.4f}_P{epoch_psnr:.2f}_S{epoch_ssim:.4f}.png",
        )
        save_triplet(preview_inp[0], preview_pred[0], preview_gt[0], img_path)
        # [Image of Triplet image: Input, Prediction, Ground Truth]

        # ======================================================
        # checkpoint ì €ì¥
        # ======================================================
        ckpt_path = os.path.join(
            cfg.save_root,
            f"epoch_{epoch:03d}_L{epoch_loss:.4f}_P{epoch_psnr:.2f}_S{epoch_ssim:.4f}.pth",
        )
        torch.save(
            {
                "epoch": epoch,
                "state_dict": model.state_dict(),
                "optimizer": optimizer.state_dict(),
                "scheduler": scheduler.state_dict(),
                # "best_ssim": best_ssim # best_ssimë„ ì €ì¥í•  ìˆ˜ ìˆìœ¼ë‚˜, ì—¬ê¸°ì„œëŠ” íŒŒì¼ ì´ë¦„ ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬
            },
            ckpt_path,
        )

        # ìµœê³  SSIM ëª¨ë¸ ì €ì¥
        if epoch_ssim > best_ssim:
            best_ssim = epoch_ssim
            # ì£¼ì˜: ëª¨ë¸ì˜ state_dictë§Œ ì €ì¥í•©ë‹ˆë‹¤.
            torch.save(model.state_dict(), os.path.join(cfg.save_root, "best_phase1_backbone.pth"))
            print("[BEST] Updated best SSIM model")

    print("\nTraining Finished.")
    print("Best SSIM:", best_ssim)


if __name__ == "__main__":
    train_phase1() """