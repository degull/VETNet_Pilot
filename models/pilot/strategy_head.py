# G:/VETNet_pilot/models/pilot/strategy_head.py
"""
StrategyHead: CLIP Vision Encoder + (Optional) LLM Strategy Generator

- ì…ë ¥:  ì´ë¯¸ì§€ í…ì„œ (B, 3, H, W), 0~1 ë²”ìœ„ ê°€ì •
- ì¶œë ¥:
    - strategy_tokens: (B, K, C_token)
    - strategy_vector: (B, D_z)
    - strategy_texts : List[str] ë˜ëŠ” None

Phase 2ì—ì„œ:
    - strategy_tokens â†’ MDTAì— concat (X; S) í˜•íƒœë¡œ ì£¼ì…
    - strategy_vector â†’ í•„ìš” ì‹œ FiLM / ì¶”ê°€ ì»¨íŠ¸ë¡¤ì— ì‚¬ìš©
    - strategy_texts â†’ XAI / ë¡œê·¸ / ë¶„ì„ìš©
"""
from __future__ import annotations

import os
import sys
from dataclasses import dataclass
from typing import Optional, List, Dict, Any

import torch
import torch.nn as nn
import torch.nn.functional as F

# -------------------------------------------------------------------------
# PATH ì„¤ì • (VETNet_pilot ë£¨íŠ¸ ì¶”ê°€)
# -------------------------------------------------------------------------
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))          # .../models/pilot
ROOT = os.path.dirname(os.path.dirname(CURRENT_DIR))              # .../VETNet_pilot

if ROOT not in sys.path:
    sys.path.insert(0, ROOT)

print(f"[strategy_head] ROOT = {ROOT}")

# -------------------------------------------------------------------------
# ì™¸ë¶€ ëª¨ë“ˆ import ì‹œë„
# -------------------------------------------------------------------------
try:
    from transformers import CLIPVisionModel
    HAS_TRANSFORMERS = True
except Exception as e:
    HAS_TRANSFORMERS = False
    print("[strategy_head WARNING] transformers/CLIPVisionModel ë¡œë“œ ì‹¤íŒ¨:", repr(e))
    print("  â†’ pip install transformers accelerate í•„ìš”")

# LLM ë¡œë”
try:
    from models.pilot.llm_loader import LLMConfig, load_llm
    HAS_LLM_LOADER = True
except Exception as e:
    HAS_LLM_LOADER = False
    LLMConfig = None  # type: ignore
    print("[strategy_head WARNING] llm_loader import ì‹¤íŒ¨:", repr(e))

# (ì„ íƒ) tokenizer_utils ì‚¬ìš©
try:
    from models.pilot.tokenizer_utils import build_strategy_prompt
    HAS_TOKENIZER_UTILS = True
except Exception as e:
    HAS_TOKENIZER_UTILS = False
    print("[strategy_head WARNING] tokenizer_utils import ì‹¤íŒ¨:", repr(e))


# -------------------------------------------------------------------------
# Config Dataclass
# -------------------------------------------------------------------------
@dataclass
class StrategyHeadConfig:
    """
    StrategyHead ì„¤ì •ê°’ ëª¨ìŒ.
    """
    # CLIP Vision ëª¨ë¸
    clip_model_name: str = "openai/clip-vit-large-patch14"
    clip_image_size: int = 224     # CLIP ì…ë ¥ í•´ìƒë„

    # Strategy Vector / Tokens ì°¨ì›
    strategy_dim: int = 256        # Z ì°¨ì›
    num_tokens: int = 4            # K
    token_dim: int = 64            # C_token (Backbone Stage1 dimê³¼ ë§ì¶”ë©´ ì¢‹ìŒ)

    # LLM ì‚¬ìš© ì—¬ë¶€
    enable_llm: bool = False       # ê¸°ë³¸ì€ ë” (ìì› ì ˆì•½)
    # ğŸ‘‰ íƒ€ì… ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•´ Any ì‚¬ìš© (Pylance ê²½ê³  ì œê±° ëª©ì )
    llm_config: Optional[Any] = None

    # í”„ë¡¬í”„íŠ¸ ê´€ë ¨
    default_dataset_tag: str = "Generic"
    language: str = "en"


# -------------------------------------------------------------------------
# StrategyHead
# -------------------------------------------------------------------------
class StrategyHead(nn.Module):
    """
    CLIP Vision Encoder + (Optional) LLMì„ ì´ìš©í•´
    Strategy Vector / Tokens / Textë¥¼ ìƒì„±í•˜ëŠ” ëª¨ë“ˆ.

    forward(
        img,             # (B,3,H,W), 0~1
        dataset_tag,     # "Rain100H", "CSD", ...
        extra_text,      # optional prompt context
        generate_text    # Trueë©´ LLMìœ¼ë¡œ strategy_text ìƒì„± (ëŠë¦¼)
    ) -> dict
    """

    def __init__(self, cfg: StrategyHeadConfig):
        super().__init__()
        self.cfg = cfg

        # ------------------------------
        # 1) CLIP Vision Encoder
        # ------------------------------
        if not HAS_TRANSFORMERS:
            raise ImportError(
                "[StrategyHead] transformers/CLIPVisionModel ì´ í•„ìš”í•©ë‹ˆë‹¤. "
                "pip install transformers accelerate"
            )

        print(f"[StrategyHead] Loading CLIP Vision Model: {cfg.clip_model_name}")
        self.clip = CLIPVisionModel.from_pretrained(cfg.clip_model_name)
        self.clip.eval()
        for p in self.clip.parameters():
            p.requires_grad = False     # Phase2 ì—ì„œëŠ” CLIP freeze ê¶Œì¥

        # CLIP feature dimension
        vision_dim = self.clip.config.hidden_size

        # ------------------------------
        # 2) Strategy Vector / Tokens Projection
        # ------------------------------
        self.proj_z = nn.Linear(vision_dim, cfg.strategy_dim)
        self.proj_tokens = nn.Linear(cfg.strategy_dim, cfg.num_tokens * cfg.token_dim)

        # ------------------------------
        # 3) LLM (Optional)
        # ------------------------------
        self.llm = None
        if cfg.enable_llm:
            if not HAS_LLM_LOADER:
                raise ImportError(
                    "[StrategyHead] enable_llm=True ì´ì§€ë§Œ llm_loader ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
            # llm_config íƒ€ì…ì„ Anyë¡œ ë‘” ìƒíƒœë¼ ì—¬ê¸°ì„  ê·¸ëƒ¥ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            llm_cfg = cfg.llm_config if cfg.llm_config is not None else LLMConfig()
            print(f"[StrategyHead] Loading LLM: {llm_cfg.base_model_name}")
            self.llm = load_llm(llm_cfg)
        else:
            print("[StrategyHead] LLM ë¹„í™œì„±í™” (enable_llm=False). "
                  "strategy_textëŠ” Noneìœ¼ë¡œ ë°˜í™˜ë©ë‹ˆë‹¤.")

        # CLIP ì „ì²˜ë¦¬ìš© mean/std (openai/clip-vit-large-patch14 ê¸°ì¤€)
        self.register_buffer(
            "clip_mean",
            torch.tensor([0.48145466, 0.4578275, 0.40821073]).view(1, 3, 1, 1),
            persistent=False,
        )
        self.register_buffer(
            "clip_std",
            torch.tensor([0.26862954, 0.26130258, 0.27577711]).view(1, 3, 1, 1),
            persistent=False,
        )

    # ------------------------------------------------------------------
    def _preprocess_for_clip(self, img: torch.Tensor) -> torch.Tensor:
        """
        img: (B,3,H,W), 0~1 ë²”ìœ„ ê°€ì •.
        CLIP ì…ë ¥ ì‚¬ì´ì¦ˆë¡œ resize + CLIP mean/std ì •ê·œí™”.
        """
        b, c, h, w = img.shape
        if (h, w) != (self.cfg.clip_image_size, self.cfg.clip_image_size):
            img = F.interpolate(
                img,
                size=(self.cfg.clip_image_size, self.cfg.clip_image_size),
                mode="bicubic",
                align_corners=False,
            )

        img = img.clamp(0.0, 1.0)
        img = (img - self.clip_mean) / self.clip_std
        return img

    # ------------------------------------------------------------------
    def _build_prompt(self, dataset_tag: Optional[str], extra_text: Optional[str]) -> str:
        """
        tokenizer_utils.build_strategy_promptê°€ ìˆìœ¼ë©´ ì‚¬ìš©í•˜ê³ ,
        ì—†ìœ¼ë©´ ë‚´ë¶€ default í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©.
        """
        tag = dataset_tag if dataset_tag is not None else self.cfg.default_dataset_tag

        # tokenizer_utilsê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        if HAS_TOKENIZER_UTILS:
            return build_strategy_prompt(dataset_tag=tag, extra_text=extra_text)

        # Fallback í”„ë¡¬í”„íŠ¸
        base = (
            "You are an expert in blind image restoration for rain, snow, haze, "
            "raindrop and illumination degradations. "
        )
        task = f"The current dataset is {tag}. "
        instr = (
            "Describe a restoration strategy that maximizes PSNR and SSIM, "
            "while preserving important edges, textures, and removing artifacts."
        )
        if extra_text is not None:
            return base + task + extra_text + " " + instr
        else:
            return base + task + instr

    # ------------------------------------------------------------------
    def forward(
        self,
        img: torch.Tensor,
        dataset_tag: Optional[str] = None,
        extra_text: Optional[str] = None,
        generate_text: bool = False,
    ) -> Dict[str, Any]:
        """
        img: (B,3,H,W), 0~1
        generate_text=True ì´ë©´ LLMì„ ì‚¬ìš©í•´ì„œ strategy_textë„ ìƒì„± (ëŠë ¤ì§ˆ ìˆ˜ ìˆìŒ)

        ë°˜í™˜:
            {
                "strategy_tokens": (B, K, C_token),
                "strategy_vector": (B, D_z),
                "strategy_texts": List[str] ë˜ëŠ” None
            }
        """
        device = img.device
        b = img.size(0)

        # 1) CLIP Vision Encoding
        x = self._preprocess_for_clip(img)
        with torch.no_grad():
            vision_out = self.clip(x, output_hidden_states=False)
            # pooled_output: (B, D_v)
            v = vision_out.pooler_output

        # 2) Strategy Vector & Tokens (CLIP ê¸°ë°˜)
        z = self.proj_z(v)                     # (B, D_z)
        tokens_flat = self.proj_tokens(z)      # (B, K*C_token)
        tokens = tokens_flat.view(
            b,
            self.cfg.num_tokens,
            self.cfg.token_dim,
        )                                      # (B, K, C_token)

        # 3) Optional: LLMìœ¼ë¡œ Strategy Text ìƒì„±
        strategy_texts: Optional[List[str]] = None
        if generate_text:
            if self.llm is None:
                raise RuntimeError(
                    "[StrategyHead] generate_text=True ì´ì§€ë§Œ enable_llm=False ì…ë‹ˆë‹¤. "
                    "config.enable_llm=True ë¡œ LLMì„ í™œì„±í™”í•´ì£¼ì„¸ìš”."
                )

            strategy_texts = []
            for i in range(b):
                prompt = self._build_prompt(dataset_tag, extra_text)
                txt, _hidden = self.llm.generate_with_hidden(prompt, device=device)
                strategy_texts.append(txt)

        return {
            "strategy_tokens": tokens,
            "strategy_vector": z,
            "strategy_texts": strategy_texts,
        }


# -------------------------------------------------------------------------
# Self-test
# -------------------------------------------------------------------------
if __name__ == "__main__":
    """
    ê°„ë‹¨í•œ self-test:

    - ë”ë¯¸ ì´ë¯¸ì§€ (B=2, 3x256x256)ë¥¼ ìƒì„±
    - StrategyHead(enable_llm=False) ë¡œ í†µê³¼
    - strategy_tokens / vector shape ì¶œë ¥

    LLMì€ ê¸°ë³¸ ë¹„í™œì„±í™”ë¼, ë¬´ê±°ìš´ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì—†ì´ CLIP + Projectionë§Œ í…ŒìŠ¤íŠ¸ëœë‹¤.
    """

    print("\n[strategy_head] Self-test ì‹œì‘")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("[strategy_head] Device =", device)

    # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„± (0~1)
    dummy_img = torch.rand(2, 3, 256, 256, device=device)

    # Config: LLMì€ ì¼ë‹¨ ë”
    cfg = StrategyHeadConfig(
        clip_model_name="openai/clip-vit-large-patch14",
        clip_image_size=224,
        strategy_dim=256,
        num_tokens=4,
        token_dim=64,
        enable_llm=False,
    )

    # ëª¨ë¸ ìƒì„±
    head = StrategyHead(cfg).to(device)
    head.eval()

    with torch.no_grad():
        out = head(dummy_img, dataset_tag="Rain100H", extra_text=None, generate_text=False)

    tokens = out["strategy_tokens"]
    z = out["strategy_vector"]
    texts = out["strategy_texts"]

    print("[strategy_head] strategy_tokens shape:", tokens.shape)  # (B, K, C_token)
    print("[strategy_head] strategy_vector shape:", z.shape)       # (B, D_z)
    print("[strategy_head] strategy_texts:", texts)

    print("\n[strategy_head] Self-test ì™„ë£Œ.\n")
